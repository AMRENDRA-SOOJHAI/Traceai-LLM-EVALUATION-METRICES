# ğŸ“Š LLM Evaluation Metrics

This document explains the **most commonly used evaluation metrics for Large Language Models (LLMs)**.
These metrics help measure **fluency, accuracy, similarity, coverage, and semantic understanding** of generated text.

---

<img width="1027" height="793" alt="image" src="https://github.com/user-attachments/assets/eea55cb0-b620-4f1f-8c42-4b532d6fc20f" />


## ğŸ”¹ 1. Perplexity

### ğŸ“Œ What is Perplexity?

Perplexity measures **how confused a language model is when predicting text**.

**In simple terms:**
Perplexity tells you **how many choices, on average, the model thinks it has for the next word**.

---

### ğŸ” Interpretation

* **Low Perplexity** â†’ Model is confident & predictable
* **High Perplexity** â†’ Model is uncertain & confused

| Perplexity Value | Interpretation              |
| ---------------- | --------------------------- |
| **1 â€“ 10**       | Excellent language modeling |
| **10 â€“ 50**      | Reasonable fluency          |
| **50 â€“ 100+**    | Poor prediction             |

---

### ğŸ§® Formula

```math
Perplexity = exp\left(-\frac{1}{N} \sum_{i=1}^{N} \log P(w_i)\right)
```

**Where:**

* `N` = Total number of tokens
* `w_i` = i-th token
* `P(w_i)` = Probability assigned to token `w_i`

---

### âœ… Key Insight

Perplexity is the **exponential of the average negative log-probability** assigned to the correct next tokens.

â¡ï¸ **Lower perplexity = better language understanding**

---

Below is a **clean, README-ready Markdown section**.
You can **directly copyâ€“paste** this into your `README.md` without any changes.

---

## ğŸ”¹ 2. Fluency (Derived from Perplexity)

### ğŸ“Œ What is Fluency?

**Fluency** measures how natural, readable, and grammatically correct a modelâ€™s output feels to humans.

In this project, fluency is **automatically derived from perplexity**, mapping a raw perplexity value to an intuitive **0â€“1 score**.

* **High Fluency (â‰ˆ 1.0)** â†’ Text is smooth, natural, and easy to read
* **Low Fluency (â‰ˆ 0.0)** â†’ Text is awkward, broken, or hard to follow

---

### ğŸ” Perplexity â†’ Fluency Mapping

Perplexity is **unbounded** and difficult to interpret directly.
To make model quality easier to compare, we convert perplexity into a **bounded fluency score (0â€“1)**.

**Intuition:**

* **Very low perplexity (â‰¤ 10)** â†’ Model is very confident â†’ fluency â‰ˆ **1.0**
* **Moderate perplexity (10â€“50)** â†’ Reasonable language quality â†’ fluency in the **mid-range**
* **High perplexity (â‰¥ 100)** â†’ Model is confused â†’ fluency â‰ˆ **0.0**

---

### ğŸ§® Fluency Formula (Project Definition)

1. Compute **perplexity** for the model outputs
2. Convert perplexity to fluency using a **logistic function**

[
\text{Fluency} = \frac{1}{1 + \exp\left(0.1 \cdot (\text{Perplexity} - 30)\right)}
]

* Centered around **perplexity = 30** (rough â€œacceptableâ€ quality)
* Output is compressed into **[0, 1]**
* **Higher score = more fluent text**

> âš ï¸ This transformation is a **design choice specific to this project**, allowing clean comparison of different models using only perplexity.

---

### ğŸ“Š Example Interpretation

| Perplexity | Approx. Fluency | Interpretation                    |
| ---------: | --------------: | --------------------------------- |
|          5 |          â‰ˆ 0.97 | Extremely fluent, very natural    |
|         20 |          â‰ˆ 0.73 | Good fluency                      |
|         40 |          â‰ˆ 0.27 | Noticeable issues in wording      |
|         80 |          â‰ˆ 0.05 | Highly disfluent / unnatural text |

---

## ğŸ”¹ 3. BLEU (Bilingual Evaluation Understudy)

### ğŸ“Œ What is BLEU?

BLEU measures **how similar a generated text is to a reference (correct) text**, based on **n-gram overlap**.

**In simple terms:**
BLEU checks **how much the modelâ€™s output looks like a known good answer**.

---

### ğŸ§  How BLEU Works

BLEU evaluates **n-gram precision**:

* **Unigram** â†’ Single words
* **Bigram** â†’ 2-word sequences
* **Trigram** â†’ 3-word sequences

---

### ğŸ“Š BLEU Score Interpretation

| BLEU Score    | Meaning                 |
| ------------- | ----------------------- |
| **0.7 â€“ 1.0** | Very close to reference |
| **0.4 â€“ 0.7** | Reasonably similar      |
| **0.2 â€“ 0.4** | Weak similarity         |
| **< 0.2**     | Poor match              |

---

### ğŸ§® BLEU Formula

```math
BLEU = BP \cdot \exp\left(\sum_{n=1}^{N} w_n \log p_n\right)
```

**Where:**

* `p_n` = n-gram precision
* `w_n` = n-gram weight (usually uniform)
* `BP` = Brevity Penalty

---

### âš ï¸ Notes

* BLEU is **precision-focused**
* Penalizes overly short outputs using **Brevity Penalty**
* Less effective for paraphrasing or creative text

---

## ğŸ”¹ 4. ROUGE (Recall-Oriented Understudy for Gisting Evaluation)

### ğŸ“Œ What is ROUGE?

ROUGE measures **overlap between generated text and reference text**, with a **focus on recall**.

In GenAI tasks, **missing important information is worse than adding extra words** â€” ROUGE captures this.

---

### ğŸ” Types of ROUGE

#### ğŸ”¸ ROUGE-N (n-gram overlap)

Measures how many **n-grams from the reference** appear in the generated text.

```math
ROUGE\text{-}N = \frac{\text{Matching n-grams}}{\text{Total n-grams in reference}}
```

---

#### ğŸ”¸ ROUGE-L (Longest Common Subsequence)

* Measures the **longest common subsequence (LCS)**
* Captures **sentence structure & fluency**

---

#### ğŸ”¸ ROUGE-S (Skip-Bigram)

* Matches word pairs allowing gaps
* Useful for flexible phrasing (less common)

---

### ğŸ“Š ROUGE Metrics

| Metric        | Meaning                             |
| ------------- | ----------------------------------- |
| **Precision** | How much generated text is relevant |
| **Recall**    | How much reference text is covered  |
| **F1 Score**  | Balance between precision & recall  |

---

## ğŸ”¹ 5. BERTScore

### ğŸ“Œ What is BERTScore?

BERTScore measures **semantic similarity** using **contextual embeddings** from pretrained transformer models
(e.g., **BERT**, **RoBERTa**).

---

### ğŸš€ Why BERTScore?

Unlike BLEU or ROUGE, BERTScore:

* âœ… Does **not rely on exact word overlap**
* âœ… Understands **meaning and paraphrases**
* âœ… Works better for **modern LLM outputs**

---

### ğŸ§  How It Works

1. Encode tokens using contextual embeddings
2. Match generated tokens with reference tokens
3. Compute similarity using **cosine similarity**

---

### ğŸ“Š BERTScore Outputs

* **Precision** â€“ Semantic relevance of generated text
* **Recall** â€“ Semantic coverage of reference text
* **F1 Score** â€“ Overall semantic similarity

âœ¨ Two sentences with the same meaning but different words can still achieve a **high BERTScore**.

---

## ğŸ§  Summary Comparison

| Metric         | Focus      | Best For           |
| -------------- | ---------- | ------------------ |
| **Perplexity** | Confidence | Language modeling  |
| **BLEU**       | Precision  | Translation        |
| **ROUGE**      | Recall     | Summarization      |
| **BERTScore**  | Semantics  | Modern GenAI tasks |

---
