# ğŸ“Š LLM Evaluation Metrics

This document explains the **most commonly used evaluation metrics for Large Language Models (LLMs)**.
These metrics help measure **fluency, accuracy, similarity, coverage, and semantic understanding** of generated text.

---

<img width="1027" height="793" alt="image" src="https://github.com/user-attachments/assets/eea55cb0-b620-4f1f-8c42-4b532d6fc20f" />

## ğŸ”¹ 1. Perplexity

### ğŸ“Œ What is Perplexity?

Perplexity measures **how confused a language model is when predicting the next token**.

**In simple terms:**
It tells you **how many choices, on average, the model thinks it has for the next word**.

---

### ğŸ” Interpretation

| Perplexity Range | Meaning                          |
| ---------------- | -------------------------------- |
| **1 â€“ 10**       | Excellent language modeling      |
| **10 â€“ 50**      | Reasonable fluency               |
| **50 â€“ 100+**    | Poor prediction / high confusion |

* **Low perplexity** â†’ Model is confident
* **High perplexity** â†’ Model is uncertain

---

### ğŸ§® Formula

```text
Perplexity = exp( - (1 / N) * Î£ log P(w_i) )
```

**Where:**

* `N` = total number of tokens
* `w_i` = i-th token
* `P(w_i)` = probability assigned to the correct token

---

### âœ… Key Insight

Perplexity is the **exponential of the average negative log-probability**.

â¡ï¸ **Lower perplexity = better language modeling ability**

---

## ğŸ”¹ 2. Fluency (Derived from Perplexity)

### ğŸ“Œ What is Fluency?

Fluency measures **how natural, readable, and grammatically correct** a modelâ€™s output feels to humans.

In this project, fluency is **derived from perplexity** and normalized into a **0â€“1 score**.

* **Fluency â‰ˆ 1.0** â†’ Smooth, natural text
* **Fluency â‰ˆ 0.0** â†’ Awkward or broken text

---

### ğŸ” Perplexity â†’ Fluency Mapping

| Perplexity Level | Expected Fluency    |
| ---------------- | ------------------- |
| â‰¤ 10             | â‰ˆ 1.0 (Very fluent) |
| 10 â€“ 50          | Mid-range fluency   |
| â‰¥ 100            | â‰ˆ 0.0 (Disfluent)   |

---

### ğŸ§® Fluency Formula (Project Definition)

```text
Fluency = 1 / (1 + exp(0.1 * (Perplexity - 30)))
```

* Centered around **Perplexity = 30** (acceptable quality)
* Output is bounded between **0 and 1**
* **Higher score = more fluent text**

> âš ï¸ This is a **project-specific design choice** to make perplexity easier to compare across models.

---

### ğŸ“Š Example Interpretation

| Perplexity | Approx. Fluency | Interpretation            |
| ---------- | --------------- | ------------------------- |
| 5          | â‰ˆ 0.97          | Extremely fluent          |
| 20         | â‰ˆ 0.73          | Good fluency              |
| 40         | â‰ˆ 0.27          | Noticeable wording issues |
| 80         | â‰ˆ 0.05          | Highly disfluent          |

---

## ğŸ”¹ 3. BLEU (Bilingual Evaluation Understudy)

### ğŸ“Œ What is BLEU?

BLEU measures **how similar a generated text is to a reference text**, using **n-gram overlap**.

**In simple terms:**
BLEU checks **how close the output is to a known correct answer**.

---

### ğŸ§  How BLEU Works

* Unigram â†’ single words
* Bigram â†’ 2-word sequences
* Trigram â†’ 3-word sequences

BLEU focuses on **precision**, not recall.

---

### ğŸ“Š BLEU Score Interpretation

| BLEU Score | Meaning                 |
| ---------: | ----------------------- |
|  0.7 â€“ 1.0 | Very close to reference |
|  0.4 â€“ 0.7 | Reasonably similar      |
|  0.2 â€“ 0.4 | Weak similarity         |
|      < 0.2 | Poor match              |

---

### ğŸ§® BLEU Formula

```text
BLEU = BP * exp( Î£ (w_n * log(p_n)) )
```

**Where:**

* `p_n` = n-gram precision
* `w_n` = n-gram weight
* `BP` = brevity penalty

---

### âš ï¸ Limitations

* Precision-focused
* Penalizes paraphrasing
* Weak for creative or open-ended tasks

---

## ğŸ”¹ 4. ROUGE (Recall-Oriented Understudy for Gisting Evaluation)

### ğŸ“Œ What is ROUGE?

ROUGE measures **overlap between generated and reference text**, with a **focus on recall**.

It answers:
â¡ï¸ *Did the model cover the important information?*

---

### ğŸ” ROUGE Variants

#### ğŸ”¸ ROUGE-N (n-gram recall)

```text
ROUGE-N = Matching n-grams / Total reference n-grams
```

#### ğŸ”¸ ROUGE-L

* Based on **Longest Common Subsequence (LCS)**
* Captures sentence structure

#### ğŸ”¸ ROUGE-S

* Skip-bigram matching
* Allows gaps between words

---

### ğŸ“Š ROUGE Metrics

| Metric    | Meaning                       |
| --------- | ----------------------------- |
| Precision | Generated content relevance   |
| Recall    | Reference coverage            |
| F1 Score  | Balance of precision & recall |

---

## ğŸ”¹ 5. BERTScore

### ğŸ“Œ What is BERTScore?

BERTScore measures **semantic similarity** using **contextual embeddings** from transformer models (BERT, RoBERTa).

---

### ğŸš€ Why BERTScore?

* âœ… Understands meaning, not just words
* âœ… Handles paraphrases well
* âœ… Better for modern LLM outputs

---

### ğŸ§  How It Works

1. Encode tokens into embeddings
2. Match tokens using cosine similarity
3. Compute Precision, Recall, and F1

---

### ğŸ“Š BERTScore Outputs

| Metric    | Meaning                     |
| --------- | --------------------------- |
| Precision | Semantic relevance          |
| Recall    | Semantic coverage           |
| F1        | Overall semantic similarity |

---

## ğŸ§  Metric Summary

| Metric     | Measures            | Best Used For      |
| ---------- | ------------------- | ------------------ |
| Perplexity | Model confidence    | Language modeling  |
| Fluency    | Readability         | Human-like quality |
| BLEU       | Precision overlap   | Translation        |
| ROUGE      | Recall overlap      | Summarization      |
| BERTScore  | Semantic similarity | GenAI evaluation   |

---

If you want next:

* ğŸ§ª **Add a â€œDummy Model vs Real Modelâ€ explanation**
* ğŸ“ˆ **Add sample outputs from your pipeline**
* ğŸ— **Convert this into a research-paper style README**

Just say ğŸ‘
