# ğŸ“Š LLM Evaluation Metrics

This document explains the **most commonly used evaluation metrics for Large Language Models (LLMs)**.
These metrics help measure **fluency, accuracy, similarity, coverage, and semantic understanding** of generated text.

---

## ğŸ”¹ 1. Perplexity

### ğŸ“Œ What is Perplexity?

Perplexity measures **how confused a language model is when predicting text**.

**In simple terms:**
Perplexity tells you **how many choices, on average, the model thinks it has for the next word**.

---

### ğŸ” Interpretation

* **Low Perplexity** â†’ Model is confident & predictable
* **High Perplexity** â†’ Model is uncertain & confused

| Perplexity Value | Interpretation              |
| ---------------- | --------------------------- |
| **1 â€“ 10**       | Excellent language modeling |
| **10 â€“ 50**      | Reasonable fluency          |
| **50 â€“ 100+**    | Poor prediction             |

---

### ğŸ§® Formula

```math
Perplexity = exp\left(-\frac{1}{N} \sum_{i=1}^{N} \log P(w_i)\right)
```

**Where:**

* `N` = Total number of tokens
* `w_i` = i-th token
* `P(w_i)` = Probability assigned to token `w_i`

---

### âœ… Key Insight

Perplexity is the **exponential of the average negative log-probability** assigned to the correct next tokens.

â¡ï¸ **Lower perplexity = better language understanding**

---

## ğŸ”¹ 2. BLEU (Bilingual Evaluation Understudy)

### ğŸ“Œ What is BLEU?

BLEU measures **how similar a generated text is to a reference (correct) text**, based on **n-gram overlap**.

**In simple terms:**
BLEU checks **how much the modelâ€™s output looks like a known good answer**.

---

### ğŸ§  How BLEU Works

BLEU evaluates **n-gram precision**:

* **Unigram** â†’ Single words
* **Bigram** â†’ 2-word sequences
* **Trigram** â†’ 3-word sequences

---

### ğŸ“Š BLEU Score Interpretation

| BLEU Score    | Meaning                 |
| ------------- | ----------------------- |
| **0.7 â€“ 1.0** | Very close to reference |
| **0.4 â€“ 0.7** | Reasonably similar      |
| **0.2 â€“ 0.4** | Weak similarity         |
| **< 0.2**     | Poor match              |

---

### ğŸ§® BLEU Formula

```math
BLEU = BP \cdot \exp\left(\sum_{n=1}^{N} w_n \log p_n\right)
```

**Where:**

* `p_n` = n-gram precision
* `w_n` = n-gram weight (usually uniform)
* `BP` = Brevity Penalty

---

### âš ï¸ Notes

* BLEU is **precision-focused**
* Penalizes overly short outputs using **Brevity Penalty**
* Less effective for paraphrasing or creative text

---

## ğŸ”¹ 3. ROUGE (Recall-Oriented Understudy for Gisting Evaluation)

### ğŸ“Œ What is ROUGE?

ROUGE measures **overlap between generated text and reference text**, with a **focus on recall**.

In GenAI tasks, **missing important information is worse than adding extra words** â€” ROUGE captures this.

---

### ğŸ” Types of ROUGE

#### ğŸ”¸ ROUGE-N (n-gram overlap)

Measures how many **n-grams from the reference** appear in the generated text.

```math
ROUGE\text{-}N = \frac{\text{Matching n-grams}}{\text{Total n-grams in reference}}
```

---

#### ğŸ”¸ ROUGE-L (Longest Common Subsequence)

* Measures the **longest common subsequence (LCS)**
* Captures **sentence structure & fluency**

---

#### ğŸ”¸ ROUGE-S (Skip-Bigram)

* Matches word pairs allowing gaps
* Useful for flexible phrasing (less common)

---

### ğŸ“Š ROUGE Metrics

| Metric        | Meaning                             |
| ------------- | ----------------------------------- |
| **Precision** | How much generated text is relevant |
| **Recall**    | How much reference text is covered  |
| **F1 Score**  | Balance between precision & recall  |

---

## ğŸ”¹ 4. BERTScore

### ğŸ“Œ What is BERTScore?

BERTScore measures **semantic similarity** using **contextual embeddings** from pretrained transformer models
(e.g., **BERT**, **RoBERTa**).

---

### ğŸš€ Why BERTScore?

Unlike BLEU or ROUGE, BERTScore:

* âœ… Does **not rely on exact word overlap**
* âœ… Understands **meaning and paraphrases**
* âœ… Works better for **modern LLM outputs**

---

### ğŸ§  How It Works

1. Encode tokens using contextual embeddings
2. Match generated tokens with reference tokens
3. Compute similarity using **cosine similarity**

---

### ğŸ“Š BERTScore Outputs

* **Precision** â€“ Semantic relevance of generated text
* **Recall** â€“ Semantic coverage of reference text
* **F1 Score** â€“ Overall semantic similarity

âœ¨ Two sentences with the same meaning but different words can still achieve a **high BERTScore**.

---

## ğŸ§  Summary Comparison

| Metric         | Focus      | Best For           |
| -------------- | ---------- | ------------------ |
| **Perplexity** | Confidence | Language modeling  |
| **BLEU**       | Precision  | Translation        |
| **ROUGE**      | Recall     | Summarization      |
| **BERTScore**  | Semantics  | Modern GenAI tasks |

---
